{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required packages and establish the necessary paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-07 10:52:46 INFO     Enabling RDKit 2021.03.5 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import os, sys, logging, pickle, random, subprocess, joblib\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pubchempy as pcp\n",
    "\n",
    "# Set up base directory paths\n",
    "BASE_DIR = os.path.curdir\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"CombDrugModule/Data/\")\n",
    "\n",
    "# Add the base directory to the system path to import the custom module\n",
    "sys.path.append(BASE_DIR)\n",
    "import CombDrugModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention! Uncomment the following cells only if you need to retrain the models from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to return the best parameters for the given score and regressor combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to return the best parameters for the given score and regressor combination\n",
    "# def getBestParams(score_name, reg_name):\n",
    "#     para_feat_name = \"{}_{}\".format(score_name, reg_name)\n",
    "#     ## Define and return parameter sets based on the score and regressor names\n",
    "#     if para_feat_name == \"ZIP_XGBoost\":\n",
    "#         return {'colsample_bylevel': 1.0, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1.0}\n",
    "#     elif para_feat_name == \"Bliss_XGBoost\":\n",
    "#         return {'colsample_bylevel': 1.0, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1.0}\n",
    "#     elif para_feat_name == \"HSA_RandomForest\":\n",
    "#         return {'max_features': 0.3, 'n_estimators': 300}\n",
    "#     elif para_feat_name == \"Loewe_RandomForest\":\n",
    "#         return {'max_features': 0.3, 'n_estimators': 300}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train and save the model based on the type of feature and model combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run2SaveTrainModel(type_feat_name):\n",
    "#     # Extract score and regressor names from the combined feature string\n",
    "#     score_name, reg_name = type_feat_name.split(\"_\")\n",
    "\n",
    "#     logging.info(\"Start {} {}...\".format(score_name, reg_name))\n",
    "\n",
    "#     ## Load feature columns from a pre-saved file\n",
    "#     with open(os.path.join(DATA_DIR, \"{}_{}_feat_name\".format(score_name, reg_name)), 'rb') as f:\n",
    "#         feat_cols = pickle.load(f)\n",
    "\n",
    "#     ## Load training data from a compressed pickle file\n",
    "#     logging.info(\"Load train data...\")\n",
    "#     train_data = pd.read_pickle(os.path.join(DATA_DIR, \"train.pickle\"),\n",
    "#                                 compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\n",
    "\n",
    "#     ## Validate that all required feature columns are present in the training data\n",
    "#     if len(set(feat_cols).difference([x for x in train_data.columns if x.startswith(\"feature_\")])) > 0:\n",
    "#         raise ValueError(\"There is an error in {}\".format(\"type_feat_name\"))\n",
    "\n",
    "#     ## Filter training data to include only the relevant features and target score column\n",
    "#     train_data = train_data[feat_cols + [\"score_{}\".format(score_name)]]\n",
    "\n",
    "#     ## Extract features and target variables for training\n",
    "#     X_train, y_train = CombDrugModule.getXy(train_data, feature_prefix='feature_',\n",
    "#                                             y_col_name=\"score_{}\".format(score_name))\n",
    "#     del train_data\n",
    "\n",
    "#     ## Train the regressor model\n",
    "#     logging.info('Train {} start...'.format(reg_name))\n",
    "#     # Retrieve the best hyperparameters for the current score and regressor combination\n",
    "#     regressor_param = getBestParams(score_name, reg_name)\n",
    "#     # Initialize the regressor model\n",
    "#     model = CombDrugModule.getRegressor(reg_name,\n",
    "#                                         regressor_params=regressor_param)\n",
    "#     ## Fit the model to the training data\n",
    "#     logging.info('Train...')\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     ## Save the trained model to disk\n",
    "#     logging.info('Save {} model...'.format(reg_name))\n",
    "#     joblib.dump(model, os.path.join(DATA_DIR, \"{}_{}_train_model\".format(score_name, reg_name)))\n",
    "\n",
    "#     logging.info(\"Done {} {}...\".format(score_name, reg_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the trained models with the optimized sets of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of feature and model combinations to process\n",
    "# for item in [\"ZIP_XGBoost\", \"Bliss_XGBoost\", \"HSA_RandomForest\", \"Loewe_RandomForest\"]:\n",
    "#     run2SaveTrainModel(item)\n",
    "\n",
    "# # Log that all models have been trained and saved\n",
    "# logging.info(\"ALL DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the synergy based on the pairs of drugs' names and the cell's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an environment variable to limit OpenBLAS threads\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '2'\n",
    "# Set the pre-trained model directory\n",
    "preTrain_path = DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to retrieve the SMILES representation of a given drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDrugSmile(drug_name):\n",
    "    # Query PubChem for the compound based on drug name\n",
    "    c = pcp.get_compounds(drug_name, \"name\")\n",
    "    if len(c) > 0:\n",
    "        com_flag = c[0]\n",
    "    else:\n",
    "        # Try querying by removing any parentheses and extra text\n",
    "        item = drug_name.split(\" (\")[0]\n",
    "        c = pcp.get_compounds(item, \"name\")\n",
    "        if len(c) == 0:\n",
    "            # Raise an error if no compound is found\n",
    "            raise ValueError(\"We cannot find {} pubmed cid and corresponding SMILES!\".format(drug_name))\n",
    "        else:\n",
    "            com_flag = c[0]\n",
    "            \n",
    "    # Return the drug name, PubChem CID, and canonical SMILES\n",
    "    return [drug_name, str(com_flag.cid), str(com_flag.canonical_smiles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to predict four drug combination scores based on the given input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predFourScoreResult(input_data, in_type, pre_num):\n",
    "    \"\"\"\n",
    "    Predict four score results based on the specified score models (ZIP and Bliss).\n",
    "    Returns:\n",
    "        DataFrame: Predicted results containing drug pairs and predicted scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy the input data for training purposes\n",
    "    data_for_train = input_data.copy()\n",
    "\n",
    "    # Adjust column names based on the `in_type` value to standardize the feature names\n",
    "    if in_type == \"Row\":\n",
    "        data_for_train = data_for_train.rename(columns={'Drug_1':'Drug_row', \n",
    "                                                        'Drug_2':'Drug_col',\n",
    "                                                        'Drug_1_cid':'Drug_row_cid', \n",
    "                                                        'Drug_2_cid':\"Drug_col_cid\"})\n",
    "        logging.info(\"Concat first result ...\")\n",
    "    elif in_type == \"Col\":\n",
    "        data_for_train = data_for_train.rename(columns={'Drug_2':'Drug_row', \n",
    "                                                        'Drug_1':'Drug_col',\n",
    "                                                        'Drug_2_cid':'Drug_row_cid', \n",
    "                                                        'Drug_1_cid':\"Drug_col_cid\"})\n",
    "        logging.info(\"Concat second result ...\")\n",
    "    \n",
    "    # Load feature columns to be used in training from a predefined file\n",
    "    with open(os.path.join(preTrain_path,\n",
    "                           \"HSA_RandomForest_feat_name\"), 'rb') as f:\n",
    "        feat_cols = pickle.load(f)\n",
    "\n",
    "    # Concatenate features by calling various feature extraction functions\n",
    "    logging.info(\"Concat all features ...\")\n",
    "    for func_name in [CombDrugModule.getInfomaxFeatureNew,\n",
    "                      CombDrugModule.getMordredFeatureNew,\n",
    "                      CombDrugModule.getRDKitFeatureNew,\n",
    "                      CombDrugModule.getgeneRawFeatureNew,\n",
    "                      CombDrugModule.getMutFeatureNew,\n",
    "                      CombDrugModule.getCNVFeatureNew]:\n",
    "        feature_df = func_name(data_for_train, pre_num, features=feat_cols)\n",
    "        data_for_train = pd.concat([data_for_train, \n",
    "                                    feature_df.reindex(data_for_train.index)], \n",
    "                                   axis=1, sort=False)\n",
    "    \n",
    "    # Release memory used by the loaded feature columns\n",
    "    del feat_cols\n",
    "    \n",
    "    # Begin predictions for each specified scoring model\n",
    "    for item in [\"ZIP_XGBoost\", \"Bliss_XGBoost\"]:\n",
    "        score_name, reg_name = item.split(\"_\")\n",
    "        logging.info(\"{} start...\".format(score_name))\n",
    "        \n",
    "        # Load the feature columns specific to the scoring model\n",
    "        with open(os.path.join(preTrain_path,\n",
    "                               \"{}_{}_feat_name\".format(score_name, reg_name)), 'rb') as f:\n",
    "            feat_cols = pickle.load(f)\n",
    "            \n",
    "        # Extract raw test data features for the current scoring model\n",
    "        raw_data = data_for_train[feat_cols].copy()\n",
    "        \n",
    "        # Load the scaling model and apply it to the test data\n",
    "        scale_tool = joblib.load(os.path.join(preTrain_path,\n",
    "                                              \"{}_{}_scale_model\".format(score_name, reg_name)))\n",
    "        \n",
    "        logging.info(\"Start impute and Scale test...\")\n",
    "        raw_data.loc[:, feat_cols] = scale_tool.transform(raw_data.loc[:, feat_cols])\n",
    "        \n",
    "        # Save the indices for the prediction results\n",
    "        test_data_index = raw_data.index\n",
    "        \n",
    "        # Extract the features for prediction\n",
    "        X_test  = raw_data.filter(regex=\"^{}\".format('feature_'), axis=1).values\n",
    "        # logging.info(\"X shape for {} is {}\".format(score_name, X_test.shape))\n",
    "        \n",
    "        # Release memory used by the raw data\n",
    "        del raw_data\n",
    "        \n",
    "        logging.info(\"Load train model...\")\n",
    "        # Load the pre-trained model for the current scoring model\n",
    "        model = joblib.load(os.path.join(preTrain_path,\n",
    "                                         \"{}_{}_train_model\".format(score_name, reg_name)))\n",
    "        \n",
    "        # Perform predictions using the loaded model\n",
    "        logging.info('Test {}...'.format(reg_name))\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Combine prediction results with the original indices\n",
    "        pred_list = []\n",
    "        for x, y in zip(test_data_index, y_pred):\n",
    "            pred_list.append([x, y])\n",
    "            \n",
    "        pred_df = pd.DataFrame(pred_list, columns=[\"DrugComb_id\", \"pred_{}\".format(score_name)])\n",
    "        pred_df = pred_df.set_index(\"DrugComb_id\")\n",
    "        \n",
    "        # Append the predicted scores to the training data\n",
    "        data_for_train = pd.concat([data_for_train, pred_df], axis=1, sort=False)\n",
    "        \n",
    "        # Release memory used by temporary variables\n",
    "        del feat_cols, pred_df, pred_list\n",
    "        \n",
    "    # Define the columns to be included in the final results\n",
    "    # columns_ = [\"Drug_row\", \"Drug_col\", \n",
    "    #                                  \"Cell_line_name\", \n",
    "    #                                  \"pred_ZIP\", \"pred_Bliss\", \n",
    "    #                                  \"pred_HSA\", \"pred_Loewe\"]\n",
    "    columns_ = [\"Drug_row\", \"Drug_col\", \n",
    "                \"Cell_line_name\", \n",
    "                \"pred_ZIP\", \"pred_Bliss\"]\n",
    "    \n",
    "    # Filter the training data to include only the relevant columns\n",
    "    data_for_train = data_for_train[columns_]\n",
    "    \n",
    "    # Save the prediction results to a CSV file\n",
    "    data_for_train.to_csv(os.path.join(CombDrugModule.TMP_DIR,\n",
    "                                       \"Drug_{}_{}\".format(in_type, pre_num)),\n",
    "                                       sep=\"\\t\",  float_format='%.3f', index=0)\n",
    "    return data_for_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the input file name from command-line arguments or use default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-07 10:53:11 INFO     'PUGREST.NotFound: No CID found that matches the given name'\n",
      "05-07 10:53:14 INFO     Starting get Infomax features...\n",
      "Using backend: pytorch\n",
      "05-07 10:53:16 INFO     Starting get Mordred features...\n",
      " 25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-07 10:53:21 INFO     Starting get RDKit features...\n",
      "05-07 10:53:22 INFO     Drug features Done...\n",
      "05-07 10:53:22 INFO     Starting get gene expression features...\n",
      "05-07 10:53:22 INFO     Starting get Mutation and CNV features...\n",
      "05-07 10:53:22 INFO     Concat first result ...\n",
      "05-07 10:53:22 INFO     Concat all features ...\n",
      "05-07 10:53:22 INFO     ZIP start...\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "05-07 10:53:22 INFO     Start impute and Scale test...\n",
      "05-07 10:53:22 INFO     Load train model...\n",
      "05-07 10:53:22 INFO     Test XGBoost...\n",
      "05-07 10:53:22 INFO     Bliss start...\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "05-07 10:53:22 INFO     Start impute and Scale test...\n",
      "05-07 10:53:23 INFO     Load train model...\n",
      "05-07 10:53:23 INFO     Test XGBoost...\n",
      "05-07 10:53:23 INFO     Concat second result ...\n",
      "05-07 10:53:23 INFO     Concat all features ...\n",
      "05-07 10:53:23 INFO     ZIP start...\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "05-07 10:53:23 INFO     Start impute and Scale test...\n",
      "05-07 10:53:24 INFO     Load train model...\n",
      "05-07 10:53:24 INFO     Test XGBoost...\n",
      "05-07 10:53:24 INFO     Bliss start...\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SimpleImputer from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/drugCombPro/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "05-07 10:53:24 INFO     Start impute and Scale test...\n",
      "05-07 10:53:24 INFO     Load train model...\n",
      "05-07 10:53:24 INFO     Test XGBoost...\n",
      "05-07 10:53:24 INFO     Concat the data...\n",
      "05-07 10:53:24 INFO     Write result...\n"
     ]
    }
   ],
   "source": [
    "# Read the input file name from command-line arguments or use default\n",
    "input_file = \"example_input.csv\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "# Get input data\n",
    "input_data = pd.read_csv(input_file)\n",
    "\n",
    "# Get unique drug list from the input data by combining both columns (Drug_1 and Drug_2)\n",
    "drug_list = list(set(input_data[\"Drug_1\"].unique()).union(set(input_data[\"Drug_2\"].unique())))\n",
    "\n",
    "# Initialize dictionaries for mapping drug names to CIDs (Chemical IDs) and CIDs to SMILES (chemical structure representation)\n",
    "drug_name2cid = {}\n",
    "drug_cid2smile = {}\n",
    "\n",
    "# Populate the dictionaries using a helper function `getDrugSmile`\n",
    "for item in drug_list:\n",
    "    drug_name_list = getDrugSmile(item)  # Example function to map drug names to SMILES\n",
    "    drug_name2cid[item] = drug_name_list[1]  # Assuming index 1 contains CID\n",
    "    drug_cid2smile[drug_name_list[1]] = drug_name_list[2]  # Assuming index 2 contains SMILES\n",
    "    \n",
    "# Map Drug_1 and Drug_2 to their respective CIDs\n",
    "input_data[\"Drug_1_cid\"] = input_data[\"Drug_1\"].map(drug_name2cid)\n",
    "input_data[\"Drug_2_cid\"] = input_data[\"Drug_2\"].map(drug_name2cid)\n",
    "\n",
    "# Load a cell line mapping file to map cell line names to their DepMap IDs\n",
    "with open(os.path.join(CombDrugModule.DATA_DIR, \"cell_line_map.pickle\"), \"rb\") as f:\n",
    "    name_map_d = pickle.load(f)\n",
    "    \n",
    "# Map cell line names to DepMap IDs in the input data\n",
    "input_data[\"DepMap_ID\"] = input_data[\"Cell_line_name\"].map(name_map_d)\n",
    "\n",
    "# Write the CID to SMILES mapping to a DataFrame\n",
    "cid_df = pd.DataFrame.from_dict(drug_cid2smile, orient='index', columns=[\"canonical_smiles\"])\n",
    "\n",
    "# Generate a random file prefix number\n",
    "pre_num = random.randint(1, 9999)\n",
    "\n",
    "# Ensure that the generated file number is unique\n",
    "while os.path.isfile(os.path.join(CombDrugModule.TMP_DIR,\n",
    "                                  \"cid2smiles_{}.pickle\".format(pre_num))):\n",
    "    pre_num = random.randint(1, 9999)\n",
    "\n",
    "# Save the CID to SMILES mapping to a pickle file\n",
    "with open(os.path.join(CombDrugModule.TMP_DIR,\n",
    "                       \"cid2smiles_{}.pickle\".format(pre_num)), \"wb\") as f:\n",
    "    pickle.dump(cid_df, f)\n",
    "\n",
    "# Run Infomax feature extraction using a shell script\n",
    "logging.info(\"Starting get Infomax features...\")\n",
    "cmd = \"bash {} {}\".format(os.path.join(CombDrugModule.MOD_DIR, \"runInfomax.sh\"),\n",
    "                          pre_num)\n",
    "subprocess.call(cmd, shell=True)\n",
    "\n",
    "# Run Mordred feature extraction\n",
    "logging.info(\"Starting get Mordred features...\")\n",
    "CombDrugModule.runForMordred(pre_num)\n",
    "\n",
    "# Run RDKit feature extraction\n",
    "logging.info(\"Starting get RDKit features...\")\n",
    "CombDrugModule.runForRDKit(pre_num)\n",
    "\n",
    "logging.info(\"Drug features Done...\")\n",
    "\n",
    "# Extract gene expression features using DepMap IDs\n",
    "logging.info(\"Starting get gene expression features...\")\n",
    "CombDrugModule.runGeneNew(list(input_data[\"DepMap_ID\"].unique()), pre_num)\n",
    "\n",
    "# Extract mutation and CNV features using DepMap IDs\n",
    "logging.info(\"Starting get Mutation and CNV features...\")\n",
    "CombDrugModule.runMutNew(list(input_data[\"DepMap_ID\"].unique()), pre_num)\n",
    "CombDrugModule.runCNVNew(list(input_data[\"DepMap_ID\"].unique()), pre_num)\n",
    "\n",
    "# Predict four scores (ZIP, Bliss) based on 'Row' and 'Col' arrangements\n",
    "data_row = predFourScoreResult(input_data, \"Row\", pre_num)\n",
    "data_col = predFourScoreResult(input_data, \"Col\", pre_num)\n",
    "\n",
    "# Concatenate predicted scores into a single DataFrame\n",
    "logging.info(\"Concat the data...\")\n",
    "# data_cols_ = [\"pred_ZIP\", \"pred_Bliss\", \"pred_HSA\", \"pred_Loewe\"]\n",
    "data_cols_ = [\"pred_ZIP\", \"pred_Bliss\",]\n",
    "data_concat = pd.concat([data_row, data_col[data_cols_]], axis=1)\n",
    "\n",
    "# Write final results to an output file\n",
    "logging.info(\"Write result...\")\n",
    "# metrics_ = [\"ZIP\", \"Bliss\", \"HSA\", \"Loewe\"]\n",
    "metrics_ = [\"ZIP\", \"Bliss\",]\n",
    "for item in metrics_:\n",
    "    input_data[item] = data_concat[\"pred_{}\".format(item)].mean(axis=1)\n",
    "    \n",
    "input_data.to_csv(output_file, sep=\",\",  float_format='%.3f', index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Drug_1 | Drug_2 | Cell_line_name | Drug_1_cid | Drug_2_cid | DepMap_ID | ZIP | Bliss |\n",
       "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
       "| Lapatinib Ditosylate (Tykerb) | Pazopanib hydrochloride | SUM-102PT | 9941095 | 11525740 | ACH-001388 | 32.899 | 60.42 |\n",
       "| Venetoclax (ABT-199) | Vincristine | SUM-102PT | 49846579 | 5978 | ACH-001388 | 31.121 | 30.944 |\n",
       "| Lapatinib Ditosylate (Tykerb) | Pazopanib hydrochloride | HCC1395 | 9941095 | 11525740 | ACH-000699 | 4.687 | 0.704 |\n",
       "| Venetoclax (ABT-199) | Vincristine | HCC1395 | 49846579 | 5978 | ACH-000699 | 9.282 | 5.284 |\n",
       "| Lapatinib Ditosylate (Tykerb) | Pazopanib hydrochloride | UACC-893 | 9941095 | 11525740 | ACH-000554 | 13.59 | 30.787 |\n",
       "| Venetoclax (ABT-199) | Vincristine | UACC-893 | 49846579 | 5978 | ACH-000554 | 7.393 | 10.184 |\n",
       "| Lapatinib Ditosylate (Tykerb) | Pazopanib hydrochloride | Du4475 | 9941095 | 11525740 | ACH-000258 | 8.056 | 11.08 |\n",
       "| Venetoclax (ABT-199) | Vincristine | Du4475 | 49846579 | 5978 | ACH-000258 | 5.27 | 4.266 |\n",
       "| Lapatinib Ditosylate (Tykerb) | Pazopanib hydrochloride | HCC1143 | 9941095 | 11525740 | ACH-000374 | -2.979 | 12.168 |\n",
       "| Venetoclax (ABT-199) | Vincristine | HCC1143 | 49846579 | 5978 | ACH-000374 | -2.481 | -0.592 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "# Convert DataFrame to Markdown-like table\n",
    "header = '| ' + ' | '.join(df.columns) + ' |'\n",
    "separator = '| ' + ' | '.join(['---'] * len(df.columns)) + ' |'\n",
    "rows = '\\n'.join(['| ' + ' | '.join(map(str, row)) + ' |' for row in df.values])\n",
    "\n",
    "# Concatenate everything\n",
    "table = f\"{header}\\n{separator}\\n{rows}\"\n",
    "\n",
    "# Display using Markdown in a cell\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "160342c7953a61afc41046b16b0c3980f32619647017b2e4d19a6e5c5b62a2a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
